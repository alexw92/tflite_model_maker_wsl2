{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163f06e7",
   "metadata": {},
   "source": [
    "# Load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load annotations without Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "curl -X GET http://url:8080/api/projects/1/export?exportType=CSV -H 'Authorization: Token xxx' --output 'annotations.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive Images (on Server!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -czvf /home/alex/all_imgs_11_10_23.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download archive from server and extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scp alex@foodsnapai.com:/home/alex/all_imgs_11_10_23.tar.gz allImgs.tar.gz\n",
    "tar -xzvf allImgs.tar.gz -C /home/alex/allImgs_extracted/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert annotations and images and preprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%run 'convert_csv_to_mlflow_csv.py' annotations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%run 'convert_pascal_to_googlecsv.py' --merged_csv annotations_mlflow.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rename the file paths in the file to their actual location (find & replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alex/tflite_model_maker_wsl2\n",
      "convert png image files to jpeg imgs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e30222a7c9f43e9a647f65fcb22673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 22\n",
      "Ignored 6206 files because: Not a PNG file\n",
      "Ignored 0 files because: Not found\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%run 'png_to_jpeg.py' annotations_mlflow_shuffled.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run preproc_imgs.py /home/alex/allImgs_extracted /home/alex/allImgs_extracted_smaller 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run normalize_csv.py annotations_mlflow_shuffled.csv annotations_mlflow_shuffled_n.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to load the model and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba09f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "spec = model_spec.get('efficientdet_lite0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26b4f5",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad66896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/alex/tflite_model_maker_wsl2\n",
      "The file '/home/alex/tflite_model_maker_wsl2/annotations_mlflow_shuffled_n.csv' exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "file_path = '/home/alex/tflite_model_maker_wsl2/annotations_mlflow_shuffled_n.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file '{file_path}' exists.\")\n",
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce5fa6",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca0f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 915s 366ms/step - det_loss: 1.5958 - cls_loss: 1.0141 - box_loss: 0.0116 - reg_l2_loss: 0.0652 - loss: 1.6610 - learning_rate: 0.0090 - gradient_norm: 1.1492 - val_det_loss: 1.4603 - val_cls_loss: 0.9380 - val_box_loss: 0.0104 - val_reg_l2_loss: 0.0652 - val_loss: 1.5255\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, validation_data=validation_data, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4952ee",
   "metadata": {},
   "source": [
    "Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18520724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae545d",
   "metadata": {},
   "source": [
    "Export the model to tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a168367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 23:24:49.461495: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 1.797 G  ops, equivalently 0.898 G  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 23:24:56.250520: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-11-12 23:24:56.250560: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-11-12 23:24:56.250718: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpo36ottr0\n",
      "2023-11-12 23:24:56.315435: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-11-12 23:24:56.315465: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpo36ottr0\n",
      "2023-11-12 23:24:56.518408: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-11-12 23:24:57.654126: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpo36ottr0\n",
      "2023-11-12 23:24:58.140511: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 1889794 microseconds.\n",
      "2023-11-12 23:25:00.081083: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1.797 G  ops, equivalently 0.898 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2023-11-12 23:26:12.019254: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1.797 G  ops, equivalently 0.898 G  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 1.797 G  ops, equivalently 0.898 G  MACs\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='models/new_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d704b3",
   "metadata": {},
   "source": [
    "Run inference script for visual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0344fa93",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "  0%|          | 0/96 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/96 [00:01<02:18,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_ac0ed1b0-d9cac84833465320_IMG_20230809_130228_HDR.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/96 [00:02<02:17,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_a651e069-4888f0ad0981b6e0_20230907_120546.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/96 [00:04<02:16,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_b33c4c6d-8c7f53cd61231cb4_20230712_184216.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/96 [00:05<02:14,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_827cf03b-573e84c3ee4a9b63_20230707_075853.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/96 [00:07<02:13,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_66daeabc-fc5c7c7143d5c8aa_F9631AE8-F84C-454E-A64C-C1681EB2A5B5.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6/96 [00:08<02:12,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_c566d9df-a508f453e78266d8_20230702_151215.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/96 [00:10<02:10,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_8ac5d141-8c497e96ed234c6a_16903118608242040818025011136096.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/96 [00:11<02:08,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_62924bda-5328b3cc4e7df0b4_0EC20CE3-4240-4715-B5A0-70C73D3A3D6C.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/96 [00:13<02:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_c76727c6-9cdd8edc90278f47_20230814_164734.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/96 [00:14<02:06,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to output_inference/prediction_8f886102-f32a646689eb11ac_9408F590-421E-467E-984E-B383F2F9F21A.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/96 [00:16<02:18,  1.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/tflite_model_maker_wsl2/do_inference.py:279\u001b[0m\n\u001b[1;32m    277\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--output_dir\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m, help\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOutput dir to save predictions to\u001b[39m\u001b[39m'\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/alex/predictions\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    278\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args()\n\u001b[0;32m--> 279\u001b[0m main(args)\n",
      "File \u001b[0;32m~/tflite_model_maker_wsl2/do_inference.py:257\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    254\u001b[0m png_file_path \u001b[39m=\u001b[39m convert_to_png(file_path)\n\u001b[1;32m    256\u001b[0m \u001b[39m# Run inference and draw detection result on the local copy of the original file\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m detection_result_image \u001b[39m=\u001b[39m run_odt_and_draw_results(\n\u001b[1;32m    258\u001b[0m     png_file_path,\n\u001b[1;32m    259\u001b[0m     interpreter,\n\u001b[1;32m    260\u001b[0m     threshold\u001b[39m=\u001b[39;49mdetection_threshold\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    263\u001b[0m \u001b[39m# Save the prediction image\u001b[39;00m\n\u001b[1;32m    264\u001b[0m save_url \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction_\u001b[39m\u001b[39m{\u001b[39;00mPath(file_path)\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/tflite_model_maker_wsl2/do_inference.py:127\u001b[0m, in \u001b[0;36mrun_odt_and_draw_results\u001b[0;34m(image_path, interpreter, threshold)\u001b[0m\n\u001b[1;32m    121\u001b[0m preprocessed_image, original_image \u001b[39m=\u001b[39m preprocess_image(\n\u001b[1;32m    122\u001b[0m     image_path,\n\u001b[1;32m    123\u001b[0m     (input_height, input_width)\n\u001b[1;32m    124\u001b[0m   )\n\u001b[1;32m    126\u001b[0m \u001b[39m# Run object detection on the input image\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m results \u001b[39m=\u001b[39m detect_objects(interpreter, preprocessed_image, threshold\u001b[39m=\u001b[39;49mthreshold)\n\u001b[1;32m    129\u001b[0m \u001b[39m# Plot the detection results on the input image\u001b[39;00m\n\u001b[1;32m    130\u001b[0m original_image_np \u001b[39m=\u001b[39m original_image\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n",
      "File \u001b[0;32m~/tflite_model_maker_wsl2/do_inference.py:95\u001b[0m, in \u001b[0;36mdetect_objects\u001b[0;34m(interpreter, image, threshold)\u001b[0m\n\u001b[1;32m     92\u001b[0m signature_fn \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_signature_runner()\n\u001b[1;32m     94\u001b[0m \u001b[39m# Feed the input image to the model\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m output \u001b[39m=\u001b[39m signature_fn(images\u001b[39m=\u001b[39;49mimage)\n\u001b[1;32m     97\u001b[0m \u001b[39m# Get all outputs from the model\u001b[39;00m\n\u001b[1;32m     98\u001b[0m count \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msqueeze(output[\u001b[39m'\u001b[39m\u001b[39moutput_0\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py:258\u001b[0m, in \u001b[0;36mSignatureRunner.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mfor\u001b[39;00m input_name, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    255\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter_wrapper\u001b[39m.\u001b[39mSetTensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs[input_name], value,\n\u001b[1;32m    256\u001b[0m                                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subgraph_index)\n\u001b[0;32m--> 258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter_wrapper\u001b[39m.\u001b[39;49mInvoke(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_subgraph_index)\n\u001b[1;32m    259\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    260\u001b[0m \u001b[39mfor\u001b[39;00m output_name, output_index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run do_inference.py --input_csv annotations_mlflow_shuffled_n.csv\\\n",
    "                       --model_url models/new_model/model.tflite --output_dir output_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df379602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
