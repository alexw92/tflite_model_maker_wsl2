{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163f06e7",
   "metadata": {},
   "source": [
    "# Load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive Images (on Server!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -czf /home/alex/all_imgs_07_01_24.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b844c8",
   "metadata": {},
   "source": [
    "#### Better: Just download the delta \n",
    "(Use the last generated csv to identify new files not part of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c52b6c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scp ./annotations/cross_val/5707_cv_fold_0.csv alex@foodsnapai.com:/home/alex/data.csv\n",
    "\n",
    "# On the server\n",
    "python archive_new_files.py --csv_file data.csv --in_dir label-studio-dir/media/upload/4 --out_dir /home/alex/deltaImgs\n",
    "# this will generate delta_imgs.tar.gz ready to be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download archive from server and extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_imgs.tar.gz                             100%   44MB  89.4MB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp alex@foodsnapai.com:/home/alex/delta_imgs.tar.gz /home/alex/delta_imgs.tar.gz\n",
    "!tar -xzf /home/alex/delta_imgs.tar.gz -C /home/alex/allImgs_extracted/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load annotations without Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations/annotations_single.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1419k  100 1419k    0     0   111k      0  0:00:12  0:00:12 --:--:--  404k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the value of the environment variable\n",
    "labelstudio_token = os.getenv('LABELSTUDIO_TOKEN')\n",
    "annotations_dir = \"annotations/\"\n",
    "filename = \"annotations_single\"\n",
    "file_path = os.path.join(annotations_dir, filename + \".csv\")\n",
    "project_id_single_groceries=4\n",
    "project_id_mixed_groceries=1\n",
    "!echo {file_path} \n",
    "!curl http://foodsnapai.com:8080/api/projects/4/export?exportType=CSV -H 'Authorization: Token {labelstudio_token}' --output {file_path} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert annotations and images and preprocess them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assuming original image dir is /home/alex/allImgs_extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3241/3241 [00:01<00:00, 2402.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Rotations from 3: 36\n",
      "#Rotations from 6: 3154\n",
      "Rotations from 8: 10\n",
      "Distinct files in dataset: 3240\n",
      "File renamed to: annotations/annotations_single_3240_mlflow.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "%run 'convert_csv_to_mlflow_csv.py' {file_path} \n",
    "\n",
    "mlflow_filename = os.path.join(annotations_dir, filename + \"_mlflow.csv\") \n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(mlflow_filename)\n",
    "\n",
    "# Count distinct values in the 'path' column\n",
    "distinct_files = df['path'].unique()\n",
    "num_distinct_files = len(distinct_files)\n",
    "\n",
    "# Rename the file to include the number of images\n",
    "base_name, extension = os.path.splitext(filename)\n",
    "new_filename =os.path.join(annotations_dir, f\"{filename}_{num_distinct_files}_mlflow.csv\") \n",
    "os.rename(mlflow_filename, new_filename)\n",
    "\n",
    "print(f\"Distinct files in dataset: {num_distinct_files}\")\n",
    "print(f\"File renamed to: {new_filename}\")\n",
    "annotations_full_filename = new_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping automatic class merge.\n",
      "['Tomato', 'Cucumber', 'Bell-Pepper', 'Onion', 'Carrot', 'Banana', 'Egg', 'Scallion', 'Lemon', 'Potato', 'Zucchini', 'Garlic', 'Apple', 'Pumpkin', 'Ginger', 'Lime', 'Avocado', 'Mango', 'Broccoli', 'Chilli', 'Cauliflower', 'Eggplant', 'Cabagge', 'Orange']\n",
      "Merging classes ['Strawberry', 'Mushroom', 'Corn', 'Leek', 'Salad', 'Canned-Tomato'] to 'other'\n",
      "Train-test-validation split applied to annotations/annotations_single_3240_mlflow_shuffled.csv\n"
     ]
    }
   ],
   "source": [
    "# merge classes here\n",
    "# curret merge string: not:Tomato,Cucumber,Bell-Pepper,Onion,Carrot,Banana,Egg,Scallion,Lemon,Potato,Zucchini,Garlic,Apple,Pumpkin,Ginger\n",
    "# not:Tomato,Cucumber,Bell-Pepper,Onion,Carrot,Banana,Egg,Scallion,Lemon,Potato,Zucchini,Garlic,Apple,Pumpkin,Ginger,Lime,Avocado,Mango,Broccoli,Chilli,Cauliflower,Eggplant,Cabagge,Orange\n",
    "# annotations_full_filename = \"annotations_single\"    +\"_mlflow.csv\"\n",
    "%run 'convert_pascal_to_googlecsv.py' --merged_csv {annotations_full_filename} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rename the file paths in the file to their actual location (find & replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'annotations/annotations_single_3240_mlflow_shuffled.csv' modified with constant path replacing each file path in the second column.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Constant path to prepend\n",
    "constant_path = '/home/alex/allImgs_extracted_smaller/'\n",
    "\n",
    "# CSV file path\n",
    "csv_dir, csv_filename = os.path.split(annotations_full_filename)\n",
    "csv_base, csv_ext = os.path.splitext(csv_filename)\n",
    "# add shuffled suffix because convert_pascal_to_googlecsv.py appends this\n",
    "csv_file_path = os.path.join(csv_dir, f\"{csv_base}_shuffled{csv_ext}\")\n",
    "\n",
    "# Open the CSV file and modify the paths in the second column\n",
    "with open(csv_file_path, 'r+', newline='') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    rows = list(csv_reader)\n",
    "\n",
    "    for row in rows:\n",
    "        if len(row) > 1:  # Ensure the row has at least two columns\n",
    "            # Assuming the file path is in the second column (index 1)\n",
    "            img_file_path = row[1]\n",
    "            img_file_name = os.path.basename(img_file_path)  # Extract the filename from the original path\n",
    "            row[1] = os.path.join(constant_path, img_file_name)  # Create the new path with the constant path and filename\n",
    "\n",
    "    # Move the file pointer to the beginning\n",
    "    file.seek(0)\n",
    "\n",
    "    # Write the modified rows back to the CSV file\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV file '{csv_file_path}' modified with constant path replacing each file path in the second column.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17003a",
   "metadata": {},
   "source": [
    "This code usually does not need to run since Mobile Phones dont upload pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alex/tflite_model_maker_wsl2\n",
      "convert png image files to jpeg imgs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b37b730d9548ecbb0ef7737a6c7538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0\n",
      "Ignored 4705 files because: Not a PNG file\n",
      "Ignored 0 files because: Not found\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%run 'png_to_jpeg.py' {csv_file_path} \n",
    "#%run 'png_to_jpeg.py' annotations1650_mlflow_shuffled_n.csv execute this line after training session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing images:  15%|█▌        | 1299/8573 [00:04<00:28, 257.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing e82677308afcf9cb_IMG_20230622_114747.jpg: cannot identify image file '/home/alex/allImgs_extracted/e82677308afcf9cb_IMG_20230622_114747.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing images:  45%|████▍     | 3839/8573 [00:12<00:17, 271.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 2ea95cbde7c6aca9_IMG_20230622_115029.jpg: cannot identify image file '/home/alex/allImgs_extracted/2ea95cbde7c6aca9_IMG_20230622_115029.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing images: 100%|██████████| 8573/8573 [00:29<00:00, 285.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8077 images did not need resizing. Copied to /home/alex/allImgs_extracted_smaller/4059e4cdc4670770_20231012_131640.jpg\n",
      "494 did need resizing. Copied to /home/alex/allImgs_extracted_smaller/4059e4cdc4670770_20231012_131640.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make images smaller (run only when new images have been loaded)\n",
    "\n",
    "%run preproc_imgs.py /home/alex/allImgs_extracted /home/alex/allImgs_extracted_smaller 800\n",
    "#%run preproc_imgs.py /mnt/z/annotated_individual/images/ /home/alex/individual_tiny_extracted_smaller 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir, file_name = os.path.split(csv_file_path)\n",
    "file_base, ext = os.path.splitext(file_name)\n",
    "\n",
    "normalized_csv = os.path.join(dir, f\"{file_base}_n{ext}\")\n",
    "\n",
    "%run normalize_csv.py {csv_file_path}  {normalized_csv} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the stats of the dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6208 different files in this set\n",
      "There are 25 different classes in this set\n",
      "Classes: Scallion, Lime, Potato, Cabagge, other, Carrot, Tomato, Orange, Cucumber, Chilli, Lemon, Broccoli, Onion, Apple, Ginger, Banana, Bell-Pepper, Garlic, Egg, Zucchini, Pumpkin, Cauliflower, Eggplant, Mango, Avocado\n",
      "Scallion occurs in 695 different files\n",
      "Lime occurs in 462 different files\n",
      "Potato occurs in 606 different files\n",
      "Cabagge occurs in 465 different files\n",
      "other occurs in 1526 different files\n",
      "Carrot occurs in 672 different files\n",
      "Tomato occurs in 1023 different files\n",
      "Orange occurs in 472 different files\n",
      "Cucumber occurs in 956 different files\n",
      "Chilli occurs in 532 different files\n",
      "Lemon occurs in 716 different files\n",
      "Broccoli occurs in 240 different files\n",
      "Onion occurs in 1125 different files\n",
      "Apple occurs in 512 different files\n",
      "Ginger occurs in 609 different files\n",
      "Banana occurs in 733 different files\n",
      "Bell-Pepper occurs in 1068 different files\n",
      "Garlic occurs in 729 different files\n",
      "Egg occurs in 366 different files\n",
      "Zucchini occurs in 741 different files\n",
      "Pumpkin occurs in 356 different files\n",
      "Cauliflower occurs in 143 different files\n",
      "Eggplant occurs in 311 different files\n",
      "Mango occurs in 165 different files\n",
      "Avocado occurs in 390 different files\n",
      "\n",
      "Class Distribution in Splits:\n",
      "\n",
      "TRAIN:\n",
      "Scallion: 542 files (4.38%)\n",
      "Lime: 375 files (3.03%)\n",
      "Potato: 474 files (3.83%)\n",
      "Cabagge: 360 files (2.91%)\n",
      "other: 1233 files (9.95%)\n",
      "Carrot: 533 files (4.3%)\n",
      "Tomato: 819 files (6.61%)\n",
      "Orange: 383 files (3.09%)\n",
      "Cucumber: 752 files (6.07%)\n",
      "Chilli: 429 files (3.46%)\n",
      "Lemon: 579 files (4.67%)\n",
      "Broccoli: 197 files (1.59%)\n",
      "Onion: 903 files (7.29%)\n",
      "Apple: 423 files (3.41%)\n",
      "Ginger: 501 files (4.04%)\n",
      "Banana: 582 files (4.7%)\n",
      "Bell-Pepper: 830 files (6.7%)\n",
      "Garlic: 557 files (4.5%)\n",
      "Egg: 289 files (2.33%)\n",
      "Zucchini: 574 files (4.63%)\n",
      "Pumpkin: 273 files (2.2%)\n",
      "Cauliflower: 106 files (0.86%)\n",
      "Eggplant: 238 files (1.92%)\n",
      "Mango: 133 files (1.07%)\n",
      "Avocado: 303 files (2.45%)\n",
      "\n",
      "TEST:\n",
      "Onion: 107 files (6.69%)\n",
      "Bell-Pepper: 124 files (7.75%)\n",
      "Zucchini: 67 files (4.19%)\n",
      "Garlic: 88 files (5.5%)\n",
      "Mango: 11 files (0.69%)\n",
      "Avocado: 40 files (2.5%)\n",
      "Cauliflower: 20 files (1.25%)\n",
      "Chilli: 54 files (3.38%)\n",
      "Lemon: 75 files (4.69%)\n",
      "Cabagge: 59 files (3.69%)\n",
      "Cucumber: 98 files (6.12%)\n",
      "Banana: 82 files (5.12%)\n",
      "Ginger: 50 files (3.12%)\n",
      "Potato: 62 files (3.88%)\n",
      "Carrot: 70 files (4.38%)\n",
      "Tomato: 101 files (6.31%)\n",
      "Orange: 55 files (3.44%)\n",
      "Lime: 47 files (2.94%)\n",
      "Pumpkin: 40 files (2.5%)\n",
      "Broccoli: 22 files (1.38%)\n",
      "other: 140 files (8.75%)\n",
      "Apple: 37 files (2.31%)\n",
      "Eggplant: 41 files (2.56%)\n",
      "Scallion: 71 files (4.44%)\n",
      "Egg: 39 files (2.44%)\n",
      "\n",
      "VALIDATE:\n",
      "Cucumber: 106 files (6.52%)\n",
      "Bell-Pepper: 114 files (7.02%)\n",
      "Chilli: 49 files (3.02%)\n",
      "Potato: 70 files (4.31%)\n",
      "Zucchini: 100 files (6.15%)\n",
      "Banana: 69 files (4.25%)\n",
      "Cabagge: 46 files (2.83%)\n",
      "Tomato: 103 files (6.34%)\n",
      "other: 153 files (9.42%)\n",
      "Lime: 40 files (2.46%)\n",
      "Scallion: 82 files (5.05%)\n",
      "Pumpkin: 43 files (2.65%)\n",
      "Apple: 52 files (3.2%)\n",
      "Lemon: 62 files (3.82%)\n",
      "Onion: 115 files (7.08%)\n",
      "Orange: 34 files (2.09%)\n",
      "Broccoli: 21 files (1.29%)\n",
      "Garlic: 84 files (5.17%)\n",
      "Carrot: 69 files (4.25%)\n",
      "Mango: 21 files (1.29%)\n",
      "Egg: 38 files (2.34%)\n",
      "Cauliflower: 17 files (1.05%)\n",
      "Ginger: 58 files (3.57%)\n",
      "Avocado: 47 files (2.89%)\n",
      "Eggplant: 32 files (1.97%)\n"
     ]
    }
   ],
   "source": [
    "#annotations_base_filename = \"annotations1900\"\n",
    "normalized_csv = './annotations/annotations_combined_6209.csv '\n",
    "%run 'get_label_stats.py' {normalized_csv} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0bc4c",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "You can also create cross validation folds.\n",
    "This script will create balanced folds and return 5 runable Datasets with distinct eval folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b6e73c1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Number of unique images: 1242\n",
      "Number of images: 4947\n",
      "Label Distribution:\n",
      "other          760\n",
      "Tomato         504\n",
      "Onion          368\n",
      "Bell-Pepper    320\n",
      "Potato         276\n",
      "Garlic         261\n",
      "Cucumber       212\n",
      "Zucchini       205\n",
      "Lemon          203\n",
      "Apple          189\n",
      "Carrot         187\n",
      "Egg            176\n",
      "Chilli         154\n",
      "Scallion       149\n",
      "Ginger         143\n",
      "Orange         142\n",
      "Lime           141\n",
      "Banana         136\n",
      "Cabagge         84\n",
      "Pumpkin         81\n",
      "Avocado         78\n",
      "Eggplant        74\n",
      "Broccoli        50\n",
      "Mango           30\n",
      "Cauliflower     24\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Fold 1:\n",
      "Number of unique images: 1242\n",
      "Number of images: 5068\n",
      "Label Distribution:\n",
      "other          760\n",
      "Tomato         454\n",
      "Onion          365\n",
      "Bell-Pepper    355\n",
      "Potato         279\n",
      "Garlic         262\n",
      "Lemon          232\n",
      "Cucumber       225\n",
      "Egg            212\n",
      "Zucchini       194\n",
      "Carrot         190\n",
      "Apple          161\n",
      "Banana         157\n",
      "Orange         156\n",
      "Scallion       144\n",
      "Chilli         143\n",
      "Lime           140\n",
      "Ginger         133\n",
      "Avocado        104\n",
      "Cabagge        102\n",
      "Eggplant        87\n",
      "Pumpkin         85\n",
      "Broccoli        58\n",
      "Mango           37\n",
      "Cauliflower     33\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Fold 2:\n",
      "Number of unique images: 1242\n",
      "Number of images: 5083\n",
      "Label Distribution:\n",
      "other          774\n",
      "Tomato         448\n",
      "Bell-Pepper    345\n",
      "Onion          339\n",
      "Potato         280\n",
      "Garlic         269\n",
      "Lemon          215\n",
      "Cucumber       214\n",
      "Egg            199\n",
      "Carrot         199\n",
      "Zucchini       198\n",
      "Apple          177\n",
      "Chilli         168\n",
      "Lime           164\n",
      "Banana         160\n",
      "Scallion       158\n",
      "Ginger         150\n",
      "Orange         149\n",
      "Avocado        111\n",
      "Cabagge        108\n",
      "Pumpkin         77\n",
      "Eggplant        68\n",
      "Broccoli        46\n",
      "Mango           38\n",
      "Cauliflower     29\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Fold 3:\n",
      "Number of unique images: 1241\n",
      "Number of images: 4894\n",
      "Label Distribution:\n",
      "other          785\n",
      "Tomato         449\n",
      "Bell-Pepper    339\n",
      "Onion          328\n",
      "Garlic         258\n",
      "Potato         256\n",
      "Lemon          209\n",
      "Carrot         208\n",
      "Cucumber       204\n",
      "Egg            180\n",
      "Apple          172\n",
      "Zucchini       172\n",
      "Chilli         162\n",
      "Banana         161\n",
      "Scallion       151\n",
      "Orange         140\n",
      "Lime           139\n",
      "Ginger         128\n",
      "Cabagge        101\n",
      "Pumpkin         93\n",
      "Avocado         88\n",
      "Eggplant        58\n",
      "Broccoli        48\n",
      "Mango           34\n",
      "Cauliflower     31\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Fold 4:\n",
      "Number of unique images: 1241\n",
      "Number of images: 5155\n",
      "Label Distribution:\n",
      "other          779\n",
      "Tomato         446\n",
      "Onion          363\n",
      "Bell-Pepper    356\n",
      "Garlic         287\n",
      "Potato         272\n",
      "Cucumber       229\n",
      "Egg            221\n",
      "Apple          213\n",
      "Carrot         204\n",
      "Lemon          202\n",
      "Zucchini       202\n",
      "Chilli         172\n",
      "Orange         158\n",
      "Banana         158\n",
      "Scallion       153\n",
      "Ginger         131\n",
      "Lime           124\n",
      "Avocado        110\n",
      "Cabagge         89\n",
      "Pumpkin         85\n",
      "Eggplant        78\n",
      "Broccoli        57\n",
      "Mango           37\n",
      "Cauliflower     29\n",
      "Name: Label, dtype: int64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/tflite_model_maker_wsl2/cross_validation_fold_creator.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_data['Split'] = 'VALIDATE'\n",
      "/home/alex/tflite_model_maker_wsl2/cross_validation_fold_creator.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_data['Split'] = 'VALIDATE'\n",
      "/home/alex/tflite_model_maker_wsl2/cross_validation_fold_creator.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_data['Split'] = 'VALIDATE'\n",
      "/home/alex/tflite_model_maker_wsl2/cross_validation_fold_creator.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_data['Split'] = 'VALIDATE'\n",
      "/home/alex/tflite_model_maker_wsl2/cross_validation_fold_creator.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  validation_data['Split'] = 'VALIDATE'\n"
     ]
    }
   ],
   "source": [
    "%run cross_validation_fold_creator.py ./annotations/annotations_combined_6209.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b715d",
   "metadata": {},
   "source": [
    "### Augment Train Dataset\n",
    "Augmentation can lead to better generalization if applied only on the train dataset and\n",
    "if conservative augmentations are used.\n",
    "This script creates augmentations for already created cross validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "833f8359",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'cv2' has no attribute '_registerMatType' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/tflite_model_maker_wsl2/img_augmentation.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mia\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m augmenters \u001b[38;5;28;01mas\u001b[39;00m iaa\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/imgaug/__init__.py:7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# this contains some deprecated classes/functions pointing to the new\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# classes/functions, hence always place the other imports below this so that\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# the deprecated stuff gets overwritten as much as possible\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimgaug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentables\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maugmentables\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/imgaug/imgaug.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterable\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/cv2/__init__.py:175\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: binary extension... OK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m __collect_extra_submodules(DEBUG):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m__load_extra_py_code_for_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/cv2/__init__.py:28\u001b[0m, in \u001b[0;36m__load_extra_py_code_for_module\u001b[0;34m(base, name, enable_debug_print)\u001b[0m\n\u001b[1;32m     26\u001b[0m native_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(module_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     py_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m enable_debug_print:\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/cv2/mat_wrapper/__init__.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m Mat\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     38\u001b[0m cv\u001b[38;5;241m.\u001b[39mMat \u001b[38;5;241m=\u001b[39m Mat\n\u001b[0;32m---> 39\u001b[0m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_registerMatType\u001b[49m(Mat)\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'cv2' has no attribute '_registerMatType' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# this must run in augment_env \n",
    "%run img_augmentation.py --cv_prefix 6209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9eed0",
   "metadata": {},
   "source": [
    "### Optionally remove other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c38e6e1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines containing 'other' removed. Output written to 'annotations/cross_val/6208_cv_fold_0_no_other.csv'.\n",
      "Lines containing 'other' removed. Output written to 'annotations/cross_val/6208_cv_fold_1_no_other.csv'.\n",
      "Lines containing 'other' removed. Output written to 'annotations/cross_val/6208_cv_fold_2_no_other.csv'.\n",
      "Lines containing 'other' removed. Output written to 'annotations/cross_val/6208_cv_fold_3_no_other.csv'.\n",
      "Lines containing 'other' removed. Output written to 'annotations/cross_val/6208_cv_fold_4_no_other.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def remove_lines_with_infix(input_filename, infix_to_remove):\n",
    "    directory, file_name = os.path.split(input_filename)\n",
    "    file_base, file_extension = os.path.splitext(file_name)\n",
    "    output_filename = os.path.join(directory, file_base + \"_no_\" + infix_to_remove + file_extension)\n",
    "\n",
    "    with open(input_filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove lines containing the infix\n",
    "    filtered_lines = [line for line in lines if infix_to_remove not in line]\n",
    "\n",
    "    with open(output_filename, 'w') as file:\n",
    "        file.writelines(filtered_lines)\n",
    "\n",
    "    print(f\"Lines containing '{infix_to_remove}' removed. Output written to '{output_filename}'.\")\n",
    "\n",
    "# Example usage\n",
    "# remove_lines_with_infix('/path/to/your/file.txt', 'infix_to_remove')\n",
    "my_file = 'annotations/cross_val/aug_4904_cv_fold_4.csv'\n",
    "cv_dir = 'annotations/cross_val/'\n",
    "prefix = \"6208\"\n",
    "cv_files = [f\"{cv_dir}aug_{prefix}_cv_fold_{i}.csv\"for i in range(5)]\n",
    "for f in cv_files:\n",
    "    remove_lines_with_infix(f, \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to load the model and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba09f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "# The current code uses Focal loss which has already weighted loss because of alpha and gamma\n",
    "model_name = 'efficientdet-lite0' # EfficientDetLite1Spec must also be set accordingly!\n",
    "custom_model_dir_name = 'model_'+\"2709_test\"#str(num_distinct_files)\n",
    "epochs = 50\n",
    "batch_size = 8\n",
    "model_dir = 'models/'+model_name+'/'+custom_model_dir_name+'_e'+str(epochs)+'_b'+str(batch_size)\n",
    "#spec = model_spec.get('efficientdet_lite1')\n",
    "# check this url to check valid hparam values\n",
    "# https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/third_party/efficientdet/hparams_config.py\n",
    "spec = object_detector.EfficientDetLite0Spec( # change this also to correct model spec\n",
    "    model_name = model_name,\n",
    "    model_dir='/home/alex/checkpoints/',\n",
    "    hparams='grad_checkpoint=true,strategy=gpus',\n",
    "    epochs=epochs, batch_size=batch_size,\n",
    "    steps_per_execution=1, moving_average_decay=0,\n",
    "    var_freeze_expr='(efficientnet|fpn_cells|resample_p6)',\n",
    "    tflite_max_detections=25\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26b4f5",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error here run the png to jpeg script again. Inference seems to fuck it up ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad66896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/alex/tflite_model_maker_wsl2\n",
      "The file 'annotations/annotations_2709_mlflow_shuffled_n.csv' exists.\n"
     ]
    }
   ],
   "source": [
    "#annotations_base_filename = 'annotations1900'\n",
    "# png to jpeg needs to run again after inference\n",
    "# %run 'png_to_jpeg.py' {normalized_csv} \n",
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n",
    "# file_path = '/home/alex/tflite_model_maker_wsl2/'+annotations_base_filename +\"_mlflow_shuffled_n.csv\"\n",
    "file_path = normalized_csv\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file '{file_path}' exists.\")\n",
    "train_data, validation_data, test_data = object_detector.DataLoader.from_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce5fa6",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ca0f89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 12:49:55.991401: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 151s 486ms/step - det_loss: 1.6713 - cls_loss: 1.0918 - box_loss: 0.0116 - reg_l2_loss: 0.0671 - loss: 1.7384 - learning_rate: 0.0090 - gradient_norm: 0.9606 - val_det_loss: 1.5957 - val_cls_loss: 1.0479 - val_box_loss: 0.0110 - val_reg_l2_loss: 0.0671 - val_loss: 1.6627\n",
      "Epoch 2/50\n",
      "270/270 [==============================] - 125s 465ms/step - det_loss: 1.4725 - cls_loss: 0.9688 - box_loss: 0.0101 - reg_l2_loss: 0.0670 - loss: 1.5396 - learning_rate: 0.0100 - gradient_norm: 1.3262 - val_det_loss: 1.5093 - val_cls_loss: 0.9836 - val_box_loss: 0.0105 - val_reg_l2_loss: 0.0670 - val_loss: 1.5764\n",
      "Epoch 3/50\n",
      " 97/270 [=========>....................] - ETA: 1:27 - det_loss: 1.4225 - cls_loss: 0.9336 - box_loss: 0.0098 - reg_l2_loss: 0.0670 - loss: 1.4895 - learning_rate: 0.0100 - gradient_norm: 1.4896"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mobject_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_whole_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/examples/tensorflow_examples/lite/model_maker/pip_package/src/tensorflow_examples/lite/model_maker/core/task/object_detector.py:260\u001b[0m, in \u001b[0;36mObjectDetector.create\u001b[0;34m(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_train:\n\u001b[1;32m    259\u001b[0m   tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetraining the models...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m   \u001b[43mobject_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m   object_detector\u001b[38;5;241m.\u001b[39mcreate_model()\n",
      "File \u001b[0;32m~/examples/tensorflow_examples/lite/model_maker/pip_package/src/tensorflow_examples/lite/model_maker/core/task/object_detector.py:123\u001b[0m, in \u001b[0;36mObjectDetector.train\u001b[0;34m(self, train_data, validation_data, epochs, batch_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m train_ds, steps_per_epoch, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    120\u001b[0m     train_data, batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m validation_ds, validation_steps, val_json_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_and_steps(\n\u001b[1;32m    122\u001b[0m     validation_data, batch_size, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalidation_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_json_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/examples/tensorflow_examples/lite/model_maker/pip_package/src/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py:265\u001b[0m, in \u001b[0;36mEfficientDetModelSpec.train\u001b[0;34m(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)\u001b[0m\n\u001b[1;32m    263\u001b[0m train\u001b[38;5;241m.\u001b[39msetup_model(model, config)\n\u001b[1;32m    264\u001b[0m train\u001b[38;5;241m.\u001b[39minit_experimental(config)\n\u001b[0;32m--> 265\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, train_whole_model=True, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the model maker class list to a file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out this instead maybe dl new version first (did not work with old one) model.export_labels(model_dir+'/label_map.json')\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "label_map = model.model_spec.config.label_map.as_dict()\n",
    "# Writing the dictionary to a JSON file\n",
    "with open(model_dir+'/label_map.json', 'w') as file:\n",
    "    json.dump(label_map, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run tensorboard to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=/home/alex/checkpoints_lite1_1700imgs/ --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4952ee",
   "metadata": {},
   "source": [
    "Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18520724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 8s 141ms/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.08548653,\n",
       " 'AP50': 0.1361972,\n",
       " 'AP75': 0.09609742,\n",
       " 'APs': 0.0,\n",
       " 'APm': 0.055434335,\n",
       " 'APl': 0.10378562,\n",
       " 'ARmax1': 0.09282439,\n",
       " 'ARmax10': 0.16570528,\n",
       " 'ARmax100': 0.17741416,\n",
       " 'ARs': 0.0,\n",
       " 'ARm': 0.11349891,\n",
       " 'ARl': 0.20717403,\n",
       " 'AP_/other': 0.2532248,\n",
       " 'AP_/Potato': 0.12074429,\n",
       " 'AP_/Tomato': 0.15464021,\n",
       " 'AP_/Onion': 0.12620524,\n",
       " 'AP_/Apple': 0.09803807,\n",
       " 'AP_/Banana': 0.2172316,\n",
       " 'AP_/Pumpkin': 0.0325109,\n",
       " 'AP_/Scallion': 0.061618336,\n",
       " 'AP_/Avocado': 0.09182611,\n",
       " 'AP_/Lemon': 0.08666255,\n",
       " 'AP_/Bell-Pepper': 0.14741343,\n",
       " 'AP_/Carrot': 0.049910072,\n",
       " 'AP_/Egg': 0.13089462,\n",
       " 'AP_/Cucumber': 0.07713252,\n",
       " 'AP_/Zucchini': 0.044840463,\n",
       " 'AP_/Chilli': 0.0001142422,\n",
       " 'AP_/Lime': 0.03753449,\n",
       " 'AP_/Ginger': 0.08042903,\n",
       " 'AP_/Garlic': 0.041250616,\n",
       " 'AP_/Cabagge': 0.07485298,\n",
       " 'AP_/Mango': 0.0,\n",
       " 'AP_/Eggplant': 0.027572501,\n",
       " 'AP_/Broccoli': 0.0970297,\n",
       " 'AP_/Cauliflower': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae545d",
   "metadata": {},
   "source": [
    "### Export the model to tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a168367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 22:55:52.942000: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2024-01-07 22:56:07.102552: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 1.834 G  ops, equivalently 0.917 G  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 22:56:11.098355: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2024-01-07 22:56:11.098387: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2024-01-07 22:56:11.101489: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpe1doj34a\n",
      "2024-01-07 22:56:11.160670: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2024-01-07 22:56:11.160691: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpe1doj34a\n",
      "2024-01-07 22:56:11.366282: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2024-01-07 22:56:12.367672: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpe1doj34a\n",
      "2024-01-07 22:56:12.809680: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 1708856 microseconds.\n",
      "2024-01-07 22:56:13.708576: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-07 22:56:14.628536: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1.834 G  ops, equivalently 0.917 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2024-01-07 22:57:25.591020: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1.834 G  ops, equivalently 0.917 G  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 1.834 G  ops, equivalently 0.917 G  MACs\n",
      "exported to model to models/efficientdet-lite0/model_2488_more_classes_e50_b8\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir=model_dir)\n",
    "print(f\"exported to model to {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 254s 1s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.072127916,\n",
       " 'AP50': 0.112064324,\n",
       " 'AP75': 0.07951266,\n",
       " 'APs': 0.0,\n",
       " 'APm': 0.034183107,\n",
       " 'APl': 0.08941372,\n",
       " 'ARmax1': 0.06421327,\n",
       " 'ARmax10': 0.10355284,\n",
       " 'ARmax100': 0.105854,\n",
       " 'ARs': 0.0,\n",
       " 'ARm': 0.054981574,\n",
       " 'ARl': 0.12072811,\n",
       " 'AP_/Cabagge': 0.058613863,\n",
       " 'AP_/Cauliflower': 0.0,\n",
       " 'AP_/Lime': 0.025742574,\n",
       " 'AP_/other': 0.22228026,\n",
       " 'AP_/Chilli': 0.000990099,\n",
       " 'AP_/Tomato': 0.065596685,\n",
       " 'AP_/Cucumber': 0.01336198,\n",
       " 'AP_/Mango': 0.0,\n",
       " 'AP_/Lemon': 0.059609786,\n",
       " 'AP_/Bell-Pepper': 0.10160959,\n",
       " 'AP_/Carrot': 0.022990871,\n",
       " 'AP_/Onion': 0.12955928,\n",
       " 'AP_/Zucchini': 0.0016142919,\n",
       " 'AP_/Ginger': 0.15429042,\n",
       " 'AP_/Garlic': 0.062772274,\n",
       " 'AP_/Potato': 0.19233683,\n",
       " 'AP_/Scallion': 0.0071628676,\n",
       " 'AP_/Avocado': 0.0,\n",
       " 'AP_/Broccoli': 0.054455444,\n",
       " 'AP_/Pumpkin': 0.14257425,\n",
       " 'AP_/Banana': 0.15792875,\n",
       " 'AP_/Eggplant': 0.0,\n",
       " 'AP_/Egg': 0.20838435,\n",
       " 'AP_/Apple': 0.049195543}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "#model_dir='models/efficientdet-lite0/model_1907_e50_b16/model.tflite'\n",
    "model.evaluate_tflite(model_dir+'/model.tflite', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d704b3",
   "metadata": {},
   "source": [
    "Run inference script for visual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0344fa93",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert png image files to jpeg imgs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513fd623808d44a7a7132b50a02212a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0\n",
      "Ignored 16196 files because: Not a PNG file\n",
      "Ignored 0 files because: Not found\n",
      "Predications will be saved to output_inference12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [04:34<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# needs to be after training for some reason\n",
    "\n",
    "%run 'png_to_jpeg.py' {annotations_base_filename +\"_mlflow_shuffled_n.csv\"}\n",
    "\n",
    "\n",
    "%run do_inference.py --input_csv {annotations_base_filename +\"_mlflow_shuffled_n.csv\"}\\\n",
    "                       --model_url {model_dir}/model.tflite --output_dir output_inference12 --label_file {model_dir}/label_map.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
